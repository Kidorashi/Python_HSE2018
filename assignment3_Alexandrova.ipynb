{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Implement linear regression model for multiclass classification using pytorch.\n",
    "\n",
    "2) Implement multinomial and one-vs-rest variants on multiclass classification.\n",
    "\n",
    "3) Implement L2 relularization for your model.\n",
    "\n",
    "4) Test your model on 20newsgroups dataset. Your baseline is accuracy=0.75.\n",
    "\n",
    "5) How can we justify using accuracy score for this problem?\n",
    "\n",
    "6) What is acuraccy score for random answer for this problem?\n",
    "\n",
    "Follow #TODO in the code below.\n",
    "Feel free to add additional regularizers to your model.\n",
    "Remember, that SGD convergence is slower that lbfgs from scikit-learn. Manage your time.\n",
    "\n",
    "Usefull links:\n",
    "\n",
    "https://pytorch.org/\n",
    "\n",
    "https://gluon.mxnet.io/chapter06_optimization/gd-sgd-scratch.html\n",
    "\n",
    "(bonus) http://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as  tt\n",
    "from torch.optim import SGD\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# retrieve dataset\n",
    "data = fetch_20newsgroups()\n",
    "\n",
    "\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "#TODO some feature engineering\n",
    "# If you want to use some sparse feature vectors, pay attention to feature size.\n",
    "# While your feature matrix can be sparse, weight tensor in the model is always dense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((11314, 45000), (11314,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=5, stop_words='english', ngram_range=(1,3), max_features=45000)\n",
    "X = tfidf.fit_transform(X)\n",
    "y = np.array(y)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionNN(nn.Module):\n",
    "    \"\"\"\n",
    "    All neural networks in pytorch are descendants of nn.Module class\n",
    "    As you remember, Logistic regression is just a 1-layer neural network\n",
    "    #TODO implement multinomial logistic regression\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d, k):\n",
    "        \"\"\"\n",
    "        In the constructor we define model weights and layers\n",
    "        d: feature size\n",
    "        k: number of classes\n",
    "        \"\"\"\n",
    "        super(LogisticRegressionNN, self).__init__()\n",
    "        \n",
    "        # TODO create tensor of weights and tensor of biases\n",
    "        # initialize tensors from N(0,1) using np.random.rand\n",
    "        # W has shape (d,k)\n",
    "        # b has shape (k,)\n",
    "        # set requires_grad=True for tensors, so they can be learned during training\n",
    "        #self.W = tt.from_numpy(np.random.normal(0, 1, (d,k)))\n",
    "        #self.b = tt.from_numpy(np.random.normal(0, 1, (k,)))\n",
    "        #self.W.require_grad = True\n",
    "        #self.b.require_grad = True\n",
    "        self.W = tt.tensor(np.random.rand(d, k), dtype=tt.float32, requires_grad=True)\n",
    "        self.b = tt.tensor(np.random.rand(k,), dtype=tt.float32, requires_grad=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In this method we implement connections between neural network weights\n",
    "        x: batch feature matrix\n",
    "        returns: probability logits\n",
    "        \"\"\"\n",
    "        # TODO implement linear model without softmax\n",
    "        result = tt.mm(x.double(), self.W.double()) + self.b.double()\n",
    "        return result\n",
    "    \n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        learnable model parameters\n",
    "        \"\"\"\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionEstimator(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Logistic Regression estimator coping interface from scikit-learn\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate, n_epochs, batch_size, alpha=1, multi_class='multinomial', verbose=False):\n",
    "        \"\"\"\n",
    "        learning_rate: SGD learning rate\n",
    "        n_epochs: number of epochs\n",
    "        batch_size: size of mini-batch\n",
    "        alpha:  regularizer coef\n",
    "        multi_class: ['multinomial', 'ovr']\n",
    "        verbose:\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.alpha = alpha\n",
    "        self.multi_class = multi_class\n",
    "        self.verbose = verbose\n",
    "        self.model_nn = None\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def _train_nn(self, model, X, y):\n",
    "        \"\"\"\n",
    "        Train neural network\n",
    "        model: neural network module\n",
    "        X: - feature matrix\n",
    "        y: - target values\n",
    "        \"\"\"\n",
    "        \n",
    "        # criterion to minimize\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        # optimization algorithm\n",
    "        optimizer = tt.optim.SGD(model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        #TODO calculate number of batches, round to the ceil\n",
    "        n_batches = int(np.ceil(X.shape[0] / self.batch_size))\n",
    "\n",
    "        if self.verbose:\n",
    "            # nice progress bar\n",
    "            t_epochs = tqdm_notebook(range(self.n_epochs), desc='epochs', leave=True)\n",
    "        else:\n",
    "            t_epochs = range(self.n_epochs)\n",
    "        \n",
    "        # iterate over epochs\n",
    "        for epoch in t_epochs:\n",
    "\n",
    "            # TODO make random permutation over indices, use np.random.choice\n",
    "            indices = np.random.choice(X.shape[0], X.shape[0], replace=False) \n",
    "\n",
    "            epoch_average_loss = 0\n",
    "\n",
    "            # iterate over mini-batches\n",
    "            for j in range(n_batches):\n",
    "                \n",
    "                batch_idx = indices[j: j + self.batch_size]\n",
    "\n",
    "                #batch_idx = indices[j*self.batch_size: j*self.batch_size + self.batch_size]\n",
    "\n",
    "                # we have to wrap data into tensors before feed them to neural network\n",
    "                #TODO: batch feature float tensor. use tt.from_numpy\n",
    "                batch_x = tt.from_numpy(X[batch_idx].toarray())\n",
    "                #batch_x.require_grad = True\n",
    "                #TODO batch target long tensor. use tt.from_numpy\n",
    "                #batch_y.require_grad = True|.astype(np.int64)\n",
    "                batch_y = tt.from_numpy(y[batch_idx])\n",
    "                #batch_y.require_grad = True\n",
    "\n",
    "                # reset gradients for the new iteration\n",
    "                optimizer.zero_grad()\n",
    "                # get predictions\n",
    "                pred = model.forward(batch_x)\n",
    "\n",
    "                # cross-entropy loss\n",
    "                loss = criterion(pred, batch_y.long())\n",
    "                #TODO: add regularizer on weights\n",
    "                #param = self.model_nn.parameters()\n",
    "                #param = tt.cat((param[0], param[1].reshape(1,-1)), dim=0)\n",
    "                #loss += self.alpha * np.linalg.norm(pred)\n",
    "                loss += self.alpha/2 * tt.norm(model.W.double())**2\n",
    "                \n",
    "\n",
    "                # calculate gradients\n",
    "                halp = tt.ones(1, dtype=tt.float64, requires_grad=True)*0.9\n",
    "                \n",
    "                loss = loss + halp\n",
    "                loss.backward()\n",
    "                # make optimization step\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_average_loss += loss.data.detach().item()\n",
    "\n",
    "            # average loss for epoch\n",
    "            epoch_average_loss /= n_batches\n",
    "            if self.verbose:\n",
    "                t_epochs.set_postfix(loss='%.3f' % epoch_average_loss)\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: feature matrix\n",
    "        y: target values\n",
    "        \"\"\"\n",
    "        \n",
    "        n_features = X.shape[1]\n",
    "        self.n_classes_ = len(np.unique(y))\n",
    "        \n",
    "        # binary classification\n",
    "        if self.n_classes_ == 2:\n",
    "            self.model_nn = LogisticRegressionNN(n_features, 2)\n",
    "            self._train_nn(self.model_nn, X, y)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            if self.multi_class == 'multinomial':\n",
    "                # TODO: multinomial classification\n",
    "                self.model_nn = LogisticRegressionNN(n_features, self.n_classes_)\n",
    "                self._train_nn(self.model_nn, X, y)\n",
    "                \n",
    "            # ovr classification\n",
    "            elif self.multi_class == 'ovr':\n",
    "                \n",
    "                if self.verbose:\n",
    "                    t_ovr = tqdm_notebook(range(self.n_classes_), desc='ovr')\n",
    "                else:\n",
    "                    t_ovr = range(self.n_classes_)\n",
    "                \n",
    "                # TODO: one-vs-rest classification\n",
    "                for key, item in enumerate(t_ovr):\n",
    "                    self.model_nn.append(LogisticRegressionNN(n_features, 2))\n",
    "                    self._train_nn(self.model_nn, X, item)\n",
    "                    \n",
    "        return self\n",
    "                    \n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        if sparse.issparse(X):\n",
    "            # create sparse tensor\n",
    "            X = X.tocoo()\n",
    "            ii = tt.LongTensor([X.row, X.col])\n",
    "            X = tt.sparse.FloatTensor(ii, tt.from_numpy(X.data).float(), X.shape)\n",
    "        else:\n",
    "            # create dense tensor\n",
    "            X = tt.from_numpy(X).float()\n",
    "            \n",
    "        \n",
    "        if self.n_classes_ == 2:\n",
    "            pred = self.model_nn.forward(X)\n",
    "            pred = tt.softmax(pred, dim=-1)\n",
    "            pred = pred.detach().numpy()\n",
    "            return pred\n",
    "            \n",
    "        else:\n",
    "            if self.multi_class == 'multinomial':\n",
    "                # TODO return class probabilities\n",
    "                pred = self.model_nn.forward(X)\n",
    "                pred = tt.softmax(pred, dim=-1)\n",
    "                pred = pred.detach().numpy()\n",
    "                return pred\n",
    "                return pred\n",
    "                \n",
    "            elif self.multi_class == 'ovr':\n",
    "                # TODO return class probabilities\n",
    "                # remember to normalize probabities from different binary classification models, so they sum up to 1.0\n",
    "                result = []\n",
    "                for m in self.models:\n",
    "                    pred = m.forward(X)\n",
    "                    pred = tt.sigmoid(pred)\n",
    "                    result.append(pred.detach().numpy()[:, 1])\n",
    "                pred = np.array(result).T\n",
    "                pred = normalize(pred)\n",
    "                    \n",
    "                return pred\n",
    "            \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)\n",
    "        return proba.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\1\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "test_data = fetch_20newsgroups(subset='test')\n",
    "\n",
    "X_test = tfidf.transform(test_data['data']).todense()\n",
    "y_test = test_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 8min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "est = LogisticRegressionEstimator(learning_rate = 1, n_epochs = 1500, batch_size = 1000, alpha = 0.001, multi_class = 'multinomial', verbose = False)\n",
    "est.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.7616834838024429\n"
     ]
    }
   ],
   "source": [
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your baseline\n",
    "print('acc', metrics.accuracy_score(y_test, est.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "acc 0.7541157727031333"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
